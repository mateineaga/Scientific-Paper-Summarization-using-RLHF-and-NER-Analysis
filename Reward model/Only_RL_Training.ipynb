{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64921e7c-b805-490a-b01a-5dfc5271abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead, PPOTrainer, PPOConfig\n",
    "from transformers import T5Tokenizer\n",
    "import copy\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205fd2d1-6ecb-4561-85b5-4624f9a64e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "#\n",
    "#                       Dataset Creation FOR REINFORCEMENT LEARNING\n",
    "#                It has the original dataset structure, present in set_final\n",
    "#\n",
    "#====================================================================================\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "def dataset_create_original(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    df_train, df_valid = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "    \n",
    "    df_train.drop(['Unnamed: 0', 'Unnamed: 0.1', 'ID', \"Title\", \"summary_word_count\", \"content_word_count\", \"interval\"], axis=1, inplace=True)\n",
    "    df_valid.drop(['Unnamed: 0', 'Unnamed: 0.1', 'ID', \"Title\", \"summary_word_count\", \"content_word_count\", \"interval\"], axis=1, inplace=True)\n",
    "\n",
    "    df_train = df_train.dropna(subset=['Content', 'Summary'])\n",
    "    df_valid = df_valid.dropna(subset=['Content', 'Summary'])\n",
    "    \n",
    "    train_dataset_panda = Dataset.from_dict(df_train[:10000])\n",
    "    valid_dataset_panda = Dataset.from_dict(df_valid[:1000])\n",
    "    my_dataset_dict = DatasetDict({\"train\":train_dataset_panda,\"test\":valid_dataset_panda,\"valid\":valid_dataset_panda})\n",
    "\n",
    "    return my_dataset_dict\n",
    "\n",
    "prefix=\"summarize: \"        #Needed for T5\n",
    "max_input_length = 890      #Modify if not efficient\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [prefix + text for text in examples[\"Content\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, truncation=True,padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "path=\"rl_dataset.csv\"                        #Path to human-annotated dataset\n",
    "orig_dataset_dict = dataset_create_original(path)\n",
    "\n",
    "orig_train_dataloader = DataLoader(orig_dataset_dict[\"train\"], batch_size=batch_size, shuffle=True)          #Modify batch size if GPU allows\n",
    "orig_valid_dataloader = DataLoader(orig_dataset_dict[\"valid\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2a5a03-f3c0-4b66-a857-ffa021a29c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from trl import PreTrainedModelWrapper\n",
    "\n",
    "class T5WithValueHead(AutoModelForSeq2SeqLMWithValueHead):\n",
    "    def __init__(self, \n",
    "                 pretrained_model,  # Take preloaded model as input\n",
    "                 value_head_hidden_size=64, \n",
    "                 dropout_rate=0.1):\n",
    "        # Initialize the PreTrainedModelWrapper with the provided model\n",
    "        super().__init__(pretrained_model)  # Passing the model directly to the wrapper\n",
    "        \n",
    "        # Assuming that `pretrained_model` is an instance of T5ForConditionalGeneration\n",
    "        self.base_model = pretrained_model\n",
    "        \n",
    "        # Get the model's hidden size\n",
    "        base_hidden_size = self.base_model.config.d_model\n",
    "        \n",
    "        # Add the value head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(base_hidden_size, value_head_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(value_head_hidden_size, value_head_hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(value_head_hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels=None, \n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,  # Include decoder_attention_mask\n",
    "                return_dict=None, \n",
    "                output_attentions=None, \n",
    "                output_hidden_states=None):\n",
    "        #if labels is not None and decoder_input_ids is None:\n",
    "        #    decoder_input_ids = labels[:, :-1]  # Shift labels to create decoder_input_ids\n",
    "        # Get the model outputs\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_input_ids=decoder_input_ids,  # Pass decoder_input_ids explicitly\n",
    "            decoder_attention_mask=decoder_attention_mask,  # Pass decoder_attention_mask\n",
    "            return_dict=return_dict,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        # Extract the last hidden state\n",
    "        encoder_outputs = outputs.encoder_last_hidden_state if return_dict else outputs[0]\n",
    "        \n",
    "        # Compute the value using the multi-layer value head\n",
    "        value = self.value_head(encoder_outputs[:, 0, :])  # Use the first token's hidden state\n",
    "        \n",
    "        # Return the loss, logits, and value (for RL tasks)\n",
    "        return (outputs.loss, outputs.logits, value)\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        # Delegate generation to the base model\n",
    "        return self.base_model.generate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bccfbd-304d-4309-b495-9727f96a7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        #self.reward_head = torch.nn.Linear(base_model.config.hidden_size, 1)\n",
    "        self.reward_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(base_model.config.hidden_size, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LayerNorm(256), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)\n",
    "    )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "\n",
    "        #cls_output = outputs.encoder_last_hidden_state[:, 0, :]  # CLS token embedding     #OLD, still viable option\n",
    "        cls_output = torch.mean(outputs.encoder_last_hidden_state, dim=1)\n",
    "        rewards = self.reward_head(cls_output)\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b44c50-c378-4d6b-8ac0-f0b4a2d2a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5 = torch.load(\"./T5\")\n",
    "# print(type(t5))\n",
    "# t5.save_pretrained(\"./T5_original_from_pretrained\")\n",
    "# model_with_value_head = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"./T5_original_from_pretrained\")\n",
    "# model_with_value_head.save_pretrained(\"./T5_with_value_head\")\n",
    "# #tokenizer = T5Tokenizer.from_pretrained(\"./finetuned_reward_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25cf2f8-9e4b-45dc-b40f-671bed265826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del t5\n",
    "# del model_with_value_head\n",
    "# del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6535caf4-c85f-4e8b-aeb3-0df70bee8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2347bf88-0bc1-4dcf-8ac0-f8e148029d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del t5\n",
    "# del reference_base_model\n",
    "# del model_with_value_head\n",
    "# del loaded_reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f2df6-2294-4440-8ea5-5bf07b338091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading correct models\n",
    "# #AFTER COMPONENTS ARE CREATED<\n",
    "# #COMPONENT 1: ORIGINAL T5 WITH VALUE HEAD (CONVERTED T5ForConditionalGeneration to AutoModelForSeq2SeqLMWithHead)\n",
    "model_with_value_head = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"./T5_with_value_head\")\n",
    "# #COMPONENT 1: ORIGINAL T5 WITH VALUE HEAD (CONVERTED T5ForConditionalGeneration to AutoModelForSeq2SeqLMWithHead)\n",
    "# t5 = T5ForConditionalGeneration.from_pretrained(\"./T5_original_from_pretrained\")\n",
    "#model_with_value_head = T5WithValueHead(t5)\n",
    "# #COMPONENT 2: from_pretrained tokenizer (\"./final_tokenizer_pretrained.pt\")                    \n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./final_tokenizer_pretrained.pt\")                  #Or the original tokenizer\n",
    "# #COMPONENT 3: Saved fine-tuned reward model\n",
    "loaded_reward_model = torch.load(\"reward_model_saved.pt\")\n",
    "# #COMPONENT 4: reference model  copy of the original T5 WITH VALUE HEAD)\n",
    "#reference_base_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"./T5_with_value_head\")     #FOR KL divergence \n",
    "\n",
    "\n",
    "model_with_value_head = model_with_value_head.to(device)\n",
    "loaded_reward_model = loaded_reward_model.to(device)\n",
    "#reference_base_model = reference_base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef84f70-4dfa-49d4-b9e5-b109590bbe70",
   "metadata": {},
   "source": [
    "## Tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5836dc9c-1835-4c3f-a5a8-f2e568c153ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = copy.deepcopy(loaded_reward_model.base_model)  # Extract the base model\n",
    "# optimizer = AdamW(base_model.parameters(), lr=5e-5)\n",
    "# criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235632ee-d14e-4bbd-8bb0-e475f7ebddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGGING\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"./RL_logs\"  # Path where TensorBoard logs will be saved\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ac05c-2d99-4470-ac41-c29ffae7166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(predictions, references)\n",
    "    return scores\n",
    "\n",
    "def validate(model, reward_model, validation_dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for batch in validation_dataloader:\n",
    "            articles = batch['Content']\n",
    "            summaries = batch['Summary']\n",
    "            \n",
    "            input_text =[\"summarize: \" + article + \" TL;DR \" for article in articles]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=890, truncation=True, padding=\"max_length\")  \n",
    "            labels = tokenizer(summaries, return_tensors=\"pt\", max_length=890, truncation=True, padding=\"max_length\")  \n",
    "\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            \n",
    "            # Get the labels and ensure they are also on the same device\n",
    "            labels = labels['input_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # Compute loss (e.g., cross-entropy loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #TODO Verify if outputs[1] is the loss\n",
    "            loss = outputs[1]    #or outputs[0]???                                  \n",
    "            #loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            # Compute ROUGE\n",
    "            predictions = outputs[0].argmax(dim=-1)\n",
    "            #predictions = outputs.logits.argmax(dim=-1)\n",
    "            predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "\n",
    "    rouge_scores = [calculate_rouge(pred, ref) for pred, ref in zip(all_predictions, all_references)]\n",
    "    \n",
    "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    avg_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    \n",
    "    avg_loss = total_loss / len(validation_dataloader)\n",
    "    return avg_loss, avg_rouge1*100, avg_rouge2*100, avg_rougeL*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9917bf4c-72e0-43a9-abed-ed493b763f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress a specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"KL divergence is starting to become negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597f81bd-5f2d-4325-9a75-1e0eb2956f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler, SequentialLR, StepLR\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            warmup_factor = float(self.last_epoch) / float(max(1, self.warmup_steps))\n",
    "            return [base_lr * warmup_factor for base_lr in self.base_lrs]\n",
    "        return self.base_lrs\n",
    "\n",
    "accumulation_steps = 1\n",
    "\n",
    "# Define the optimizer with a base learning rate\n",
    "optimizer = optim.AdamW(model_with_value_head.parameters(), lr=4e-6)\n",
    "\n",
    "# Create the warmup scheduler\n",
    "warmup_steps = int(50/accumulation_steps)\n",
    "warmup_scheduler = WarmUpLR(optimizer, warmup_steps)\n",
    "\n",
    "# Define the step scheduler after warm-up\n",
    "step_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(200/accumulation_steps), gamma=0.8)\n",
    "\n",
    "# Combine warmup with step scheduler\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, step_scheduler], milestones=[warmup_steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bd4b6-63b0-4448-b4c1-1bf96216b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING WITH PPO TRAINER\n",
    "num_epochs = 1\n",
    "num_return_sequences = 2     #TUNE\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=\"./T5_original_from_pretrained\",\n",
    "    reward_model=\"reward_model_saved_final.pt\",\n",
    "    learning_rate=4e-6,           #Too small? the model is already fine tuned, though\n",
    "    batch_size=batch_size*num_return_sequences,                # Number of article-summary pairs (2 art/batch * 3 summaries/art = 6)\n",
    "    ppo_epochs=3,                # Number of PPO epochs for each update\n",
    "    mini_batch_size=8,           # Mini-batch size\n",
    "    gamma=0.99,                  # Discount factor\n",
    "    max_grad_norm=0.5,           # Maximum gradient norm\n",
    ")\n",
    "\n",
    "#reference_base_model.eval()  # Freeze the reference model\n",
    "\n",
    "#ppo_trainer = PPOTrainer(config=ppo_config, model=model_with_value_head, ref_model=reference_base_model, tokenizer=tokenizer)#dataset=orig_dataset_dict[\"train\"])\n",
    "ppo_trainer = PPOTrainer(config=ppo_config, model=model_with_value_head, tokenizer=tokenizer, optimizer = optimizer)#, dataset=orig_dataset_dict[\"train\"])\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "\n",
    "  \n",
    "response_generation_kwargs = {\n",
    "    \"min_length\": 10,  # Set this to a reasonable minimum summary length\n",
    "    \"max_new_tokens\": 128,  # You can adjust max_length based on the desired summary length\n",
    "    \"top_k\": 40,  # Set this to a reasonable value (e.g., 50)\n",
    "    \"top_p\": 1.0,  # No truncation, samples from the entire distribution\n",
    "    \"do_sample\": True,  # Enable sampling for diversity\n",
    "    \"num_return_sequences\": num_return_sequences,  # Generate 3 summaries\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # Use EOS token for padding\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    reward_accumulator = 0.0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(orig_train_dataloader), total=len(orig_train_dataloader)):\n",
    "        articles = batch['Content']\n",
    "        summaries = batch['Summary']\n",
    "\n",
    "        queries = []\n",
    "        responses = []\n",
    "        rewards = []\n",
    "\n",
    "        all_responses = []\n",
    "        all_queries = []\n",
    "\n",
    "        for article, summary in zip(articles, summaries):\n",
    "            input_text = \"summarize: \" + article + \" TL;DR \"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=890, truncation=True, padding=\"max_length\")         #IF original dataset is not already tokenized\n",
    "                \n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "            #outputs = model_with_value_head.generate(input_ids, max_new_tokens=128, num_beams=3, num_return_sequences=num_return_sequences)\n",
    "            outputs = model_with_value_head.generate(input_ids, **response_generation_kwargs)\n",
    "            \n",
    "            for output in outputs:\n",
    "                candidate = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "                #Reduce the size of the article, so that both the input and summary fit\n",
    "                inputs_rew = tokenizer(input_text, return_tensors=\"pt\", max_length = 890, truncation=True)   #Placeholder; can be optimized - concatenating the article with the candidate summary using segmentation (the functions above)\n",
    "                inputs_rew = inputs_rew.to(device)\n",
    "\n",
    "                #print(input_ids.shape)\n",
    "                queries.append(input_ids.squeeze(0))\n",
    "                responses.append(output)\n",
    "                    \n",
    "                inputs_rew = tokenizer.decode(inputs_rew['input_ids'][0], skip_special_tokens=True)\n",
    "                input_for_reward = inputs_rew + \" TL; DR \" + candidate    #modify according to model\n",
    "\n",
    "                reward_inputs = tokenizer(input_for_reward, return_tensors='pt', padding='max_length', truncation=True, max_length=1024)##TODO check if  Padding and Truncation are missing in some places in the code\n",
    "                reward_inputs = reward_inputs.to(device)\n",
    "\n",
    "                #Compute the reward for every article-summary pair\n",
    "                reward = loaded_reward_model(reward_inputs['input_ids'], reward_inputs['attention_mask'], labels=reward_inputs['input_ids']) ##labels=None may not be good\n",
    "                \n",
    "                rewards.append(reward)\n",
    "        \n",
    "                all_queries.append(input_text)\n",
    "                all_responses.append(candidate)\n",
    "\n",
    "        rewards = [r[0] for r in rewards]  # Flatten the rewards list (to get shape [batch_size])\n",
    "        \n",
    "        stats = ppo_trainer.step(queries, responses, rewards)\n",
    "        scheduler.step()\n",
    "        #print(stats.keys())\n",
    "\n",
    "        loss = stats['ppo/loss/total']  # Assuming 'loss' is in the stats dictionary\n",
    "        #reward = stats['reward'].item()  # Assuming 'reward' is in the stats dictionary\n",
    "\n",
    "        reward_accumulator += stats['ppo/mean_non_score_reward']\n",
    "\n",
    "        running_loss = running_loss + loss\n",
    "        \n",
    "        if idx%3==2 and idx < 20:\n",
    "            #writer.add_scalar('Loss/train', running_loss/2, epoch * len(orig_train_dataloader) + idx)\n",
    "            #writer.add_scalar('Reward/train', reward_accumulator/2, epoch * len(orig_train_dataloader) + idx)\n",
    "            print(f\"Train Loss: {running_loss / idx:.4f} - Train reward: {reward_accumulator/idx:.4f}\")\n",
    "            #running_loss = 0.0\n",
    "            #reward_accumulator = 0.0\n",
    "\n",
    "        if idx%25==24:\n",
    "            avg_loss = running_loss / 25.0\n",
    "            reward_avg = reward_accumulator / 25.0\n",
    "            print(f\"Train Loss: {avg_loss:.4f} - Train reward: {reward_avg:.4f}\")\n",
    "            writer.add_scalar('Loss/train', avg_loss, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "            writer.add_scalar('Reward/train', reward_avg, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model_with_value_head.state_dict(),\n",
    "                'step': idx,  # Current step or epoch\n",
    "                'loss': avg_loss,  # Current loss (optional)\n",
    "                'reward': reward_avg,\n",
    "            }\n",
    "            torch.save(checkpoint, \"RL_CHECKPOINT.pt\")\n",
    "            running_loss = 0.0\n",
    "            reward_accumulator = 0.0\n",
    "\n",
    "        if idx%75==74:\n",
    "            val_loss, val_rouge1, val_rouge2, val_rougeL = validate(model_with_value_head, loaded_reward_model, orig_valid_dataloader, device)\n",
    "            print(f\"Validation Loss: {val_loss:.4f} - R1: {val_rouge1:.4f} - R2: {val_rouge2:.4f} - RL: {val_rougeL:.4f}\")\n",
    "            writer.add_scalar('Loss/valid', val_loss, epoch * len(orig_train_dataloader) + idx*6)\n",
    "            writer.add_scalar('ROUGE1/valid', val_rouge1, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "            writer.add_scalar('ROUGE2/valid', val_rouge2, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "            writer.add_scalar('ROUGEL/valid', val_rougeL, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "            \n",
    "        batch_all = {'query': all_queries, 'response': all_responses}\n",
    "        ppo_trainer.log_stats(stats, batch_all, rewards)\n",
    "\n",
    "    val_loss, val_rouge1, val_rouge2, val_rougeL = validate(model_with_value_head, loaded_reward_model, orig_valid_dataloader, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} - R1: {val_rouge1:.4f} - R2: {val_rouge2:.4f} - RL: {val_rougeL:.4f}\")\n",
    "    writer.add_scalar('Loss/valid', val_loss, epoch * len(orig_train_dataloader) + idx*6)\n",
    "    writer.add_scalar('ROUGE1/valid', val_rouge1, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "    writer.add_scalar('ROUGE2/valid', val_rouge2, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "    writer.add_scalar('ROUGEL/valid', val_rougeL, (epoch * len(orig_train_dataloader) + idx)*6)\n",
    "    \n",
    "\n",
    "torch.save(model_with_value_head, \"FINAL_FT_T5_VALUE_HEAD.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cb1e9-76f9-4789-87d4-18f3a71160d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_with_value_head, \"FINAL_FT_T5_VALUE_HEAD.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "417ed2c2-f851-4b15-a52d-e79fe12ae442",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_with_value_head.pretrained_model, \"RL_T5_FINAL.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175dfcdc-9eb0-4953-88f3-2318ba6fa300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_value_head.save_pretrained(\"fine_tuned_RL_model\", push_to_hub=False)\n",
    "tokenizer.save_pretrained(\"fine_tuned_RL_tokenizer\", push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80a742-57d0-4178-b7d4-1a232c8acc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92c2b4b-d822-40e1-91a7-802a7009a689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
