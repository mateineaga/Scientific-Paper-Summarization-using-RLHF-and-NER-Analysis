{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc5ebfb-0cec-4fec-a7af-4917d354e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a31f6f-6c26-4927-b42b-8e987359b289",
   "metadata": {},
   "source": [
    "## Tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c51f8-09d6-4478-afd5-b76758bd5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGGING\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"./phase_2_reward_model\" \n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2353df07-a9a9-4306-8569-279b4a0678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================================================\n",
    "#\n",
    "#                                   Dataset Creation - PROCESS DATASET FOR REWARD MODEL\n",
    "#               assumes csv columns: ID,Title,content,summary_1,summary_2,summary_3,reward_1,reward_2,reward_3\n",
    "#\n",
    "#==========================================================================================================================\n",
    "def dataset_create(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    df_train, df_valid = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "    df_train.drop(['Unnamed: 0.1', 'Unnamed: 0', 'ID', 'Title', 'Summary', 'summary_word_count', 'content_word_count', 'interval'], axis=1, inplace=True)\n",
    "    df_valid.drop(['Unnamed: 0.1', 'Unnamed: 0', 'ID', 'Title', 'Summary', 'summary_word_count', 'content_word_count', 'interval'], axis=1, inplace=True)\n",
    "\n",
    "    #raman cu Summary,Content,summary_1,summary_2,summary_3,reward_1,reward_2,reward_3,rewards\n",
    "\n",
    "    df_train[['reward_1', 'reward_2', 'reward_3']] = df_train['rewards'].str.split(',', expand=True)\n",
    "    df_train[['reward_1', 'reward_2', 'reward_3']] = df_train[['reward_1', 'reward_2', 'reward_3']].astype(float)\n",
    "    df_train = df_train.drop(columns=['rewards'])\n",
    "\n",
    "    df_valid[['reward_1', 'reward_2', 'reward_3']] = df_valid['rewards'].str.split(',', expand=True)\n",
    "    df_valid[['reward_1', 'reward_2', 'reward_3']] = df_valid[['reward_1', 'reward_2', 'reward_3']].astype(float)\n",
    "    df_valid = df_valid.drop(columns=['rewards'])\n",
    "\n",
    "    for col in df_train.columns:\n",
    "        if df_train[col].dtype == 'object':  # Ensure string columns are of type 'str'\n",
    "            df_train[col] = df_train[col].astype('str')\n",
    "        elif df_train[col].dtype == 'float64':  # Ensure numeric columns are floats (or another numeric type)\n",
    "            df_train[col] = df_train[col].astype('float32')\n",
    "\n",
    "    for col in df_valid.columns:\n",
    "        if df_valid[col].dtype == 'object':  # Ensure string columns are of type 'str'\n",
    "            df_valid[col] = df_valid[col].astype('str')\n",
    "        elif df_valid[col].dtype == 'float64':  # Ensure numeric columns are floats (or another numeric type)\n",
    "            df_valid[col] = df_valid[col].astype('float32')\n",
    "\n",
    "    train_dataset_panda = Dataset.from_dict(df_train)\n",
    "    valid_dataset_panda = Dataset.from_dict(df_valid[:1000])\n",
    "    my_dataset_dict = DatasetDict({\"train\":train_dataset_panda,\"valid\":valid_dataset_panda})\n",
    "    \n",
    "    return my_dataset_dict\n",
    "\n",
    "path=\"reward_dataset.csv\"                        #Path to human-annotated dataset\n",
    "dataset_dict = dataset_create(path)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_dict[\"train\"], batch_size=2, shuffle=True)            #Increase batch size, if GPU allows\n",
    "valid_dataloader = DataLoader(dataset_dict[\"valid\"], batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c3976f-0ceb-46e1-95e1-02165e826de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD A FINE-TUNED MODEL (the base for the reward model)\n",
    "model = torch.load(\"T5\")                    #Path to fine-tuned language model\n",
    "model.config.output_hidden_states = True    #IDK if needed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BlackKakapo/t5-small-grammar-ro-root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047499f6-8a55-497a-9be8-b102f540a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or ListNet, or original ListMLE loss, or Pairwise ranking loss\n",
    "def modified_listmle_loss(predicted_scores, true_scores):\n",
    "    _, sorted_indices = torch.sort(true_scores, descending=True, dim=-1)\n",
    "    sorted_predicted_scores = torch.gather(predicted_scores, dim=-1, index=sorted_indices)\n",
    "    \n",
    "    log_cumsum_exp = torch.logcumsumexp(sorted_predicted_scores, dim=-1)\n",
    "    listmle = -torch.sum(sorted_predicted_scores - log_cumsum_exp, dim=-1).mean()\n",
    "    \n",
    "    # Adauga MSE loss\n",
    "    mse = torch.nn.functional.mse_loss(predicted_scores, true_scores)\n",
    "    \n",
    "    # Combina losses\n",
    "    #regularization = torch.var(predicted_scores, dim=-1).mean()  # Penalizes collapsing neutilizat in PHASE 2\n",
    "    return listmle + 0.1 * mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a629cbb-1583-4309-8dd0-68870f23b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================================\n",
    "#\n",
    "#                    Reward Model Training\n",
    "#\n",
    "#=================================================================\n",
    "\n",
    "##TODO: DO the entire preprocessing (tokenization, splitting into max_length chunks) before the training loop\n",
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        #self.reward_head = torch.nn.Linear(base_model.config.hidden_size, 1)\n",
    "        self.reward_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(base_model.config.hidden_size, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LayerNorm(256), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)\n",
    "    )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "\n",
    "        cls_output = torch.mean(outputs.encoder_last_hidden_state, dim=1)\n",
    "        rewards = self.reward_head(cls_output)\n",
    "\n",
    "        return rewards\n",
    "    \n",
    "def eval_epoch_reward_model(reward_model, val_dataloader, criterion):\n",
    "    reward_model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    print(\"EVALUATION\")\n",
    "    with torch.no_grad():  # No gradient computation during evaluation\n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            articles = batch['Content']\n",
    "            summaries_all = [batch['summary_1'], batch['summary_2'], batch['summary_3']]      #List of lists\n",
    "            summaries_all = list(map(list, zip(*summaries_all)))\n",
    "            rewards_all = [batch['reward_1'], batch['reward_2'], batch['reward_3']]\n",
    "            rewards_all = list(map(list, zip(*rewards_all)))\n",
    "            \n",
    "            input_texts = []\n",
    "            batch_rewards = []\n",
    "\n",
    "            for article, summaries, rewards in zip(articles, summaries_all, rewards_all):\n",
    "                art_rew = []\n",
    "                for summary, reward in zip(summaries, rewards):\n",
    "                    #summary = summary[0]                        #when batch_size=1, summary is a one str list\n",
    "\n",
    "                    inputs_rew = tokenizer(article, return_tensors=\"pt\", max_length = 890, truncation=True)   #Placeholder; can be optimized - concatenating the article with the candidate summary using segmentation (the functions above)\n",
    "                    inputs_rew = inputs_rew.to(device)\n",
    "                    \n",
    "                    inputs_rew = tokenizer.decode(inputs_rew['input_ids'][0], skip_special_tokens=True)\n",
    "                    input_for_reward = \"summarize: \" + inputs_rew + \" TL; DR \" + summary    #modify according to model\n",
    "\n",
    "                    input_texts.append(input_for_reward)\n",
    "                    art_rew.append(reward)\n",
    "\n",
    "                batch_rewards.append(art_rew)\n",
    "                num_samples += 1\n",
    "                        \n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=1024)\n",
    "\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            labels = input_ids  # labels can be the same as input_ids\n",
    "\n",
    "            batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "\n",
    "            outputs = reward_model(input_ids, attention_mask, labels)\n",
    "            predicted_rewards = outputs.view(-1, 3)  # Assuming 3 summaries per article\n",
    "\n",
    "            predicted_rewards = (predicted_rewards - predicted_rewards.mean(dim=-1, keepdim=True)) / (predicted_rewards.std(dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            loss = modified_listmle_loss(predicted_rewards, batch_rewards)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "\n",
    "    # Compute the average loss over the entire validation set\n",
    "    avg_loss = total_loss / num_samples\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfce0ae2-ec8e-4a81-95db-8ac400020489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(reward_model, train_dataloader, valid_dataloader, optimizer, criterion, scheduler, accumulation_steps=4, epochs=1):\n",
    "    reward_model.train()\n",
    "\n",
    "    max_length=1024             #Move somewhere else\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Train\"):\n",
    "            articles = batch['Content']\n",
    "            summaries_all = [batch['summary_1'], batch['summary_2'], batch['summary_3']]      #List of lists\n",
    "            summaries_all = list(map(list, zip(*summaries_all)))\n",
    "            rewards_all = [batch['reward_1'], batch['reward_2'], batch['reward_3']]\n",
    "            rewards_all = list(map(list, zip(*rewards_all)))\n",
    "\n",
    "            input_texts = []\n",
    "            batch_rewards = []\n",
    "\n",
    "            for article, summaries, rewards in zip(articles, summaries_all, rewards_all):\n",
    "                art_rew = []\n",
    "                for summary, reward in zip(summaries, rewards):\n",
    "                    #summary = summary[0]                        #when batch_size=1, summary is a one str list\n",
    "\n",
    "                    inputs_rew = tokenizer(article, return_tensors=\"pt\", max_length = 890, truncation=True)   #Placeholder; can be optimized - concatenating the article with the candidate summary using segmentation (the functions above)\n",
    "                    inputs_rew = inputs_rew.to(device)\n",
    "                    \n",
    "                    inputs_rew = tokenizer.decode(inputs_rew['input_ids'][0], skip_special_tokens=True)\n",
    "                    input_for_reward = \"summarize: \" + inputs_rew + \" TL; DR \" + summary    #modify according to model\n",
    "\n",
    "                    input_texts.append(input_for_reward)\n",
    "                    art_rew.append(reward)\n",
    "                \n",
    "                batch_rewards.append(art_rew)\n",
    "                num_samples += 1\n",
    "\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=1024)\n",
    "\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            labels = input_ids  # labels can be the same as input_ids\n",
    "            \n",
    "            batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "            \n",
    "            outputs = reward_model(input_ids, attention_mask, labels)\n",
    "            predicted_rewards = outputs.view(-1, 3)  # Assuming 3 summaries per article\n",
    "\n",
    "            predicted_rewards = (predicted_rewards - predicted_rewards.mean(dim=-1, keepdim=True)) / (predicted_rewards.std(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "            loss = modified_listmle_loss(predicted_rewards, batch_rewards)\n",
    "            \n",
    "            total_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(reward_model.parameters(), max_norm=1.0)\n",
    "            # # Optimizer step\n",
    "            # optimizer.step()\n",
    "            # scheduler.step()  # Update learning rate (if scheduler is used)\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            if (idx + 1) % accumulation_steps == 0 or (idx + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(reward_model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate (if scheduler is used)\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            if idx%10==9 and idx < 100:\n",
    "                print(\"Outputs: \", outputs)\n",
    "                print(\"Predicted: \", predicted_rewards)\n",
    "                print(\"Ref: \", batch_rewards)\n",
    "                avg_loss = total_loss / num_samples\n",
    "                print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "            if idx%500==499:\n",
    "                print(\"Outputs: \", outputs)\n",
    "                print(\"Predicted: \", predicted_rewards)\n",
    "                print(\"Ref: \", batch_rewards)\n",
    "                avg_loss = total_loss / num_samples\n",
    "                writer.add_scalar('Loss/train', avg_loss, idx+len(train_dataloader)*epoch)               \n",
    "                total_loss = 0.0\n",
    "                num_samples = 0.0\n",
    "                print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "                writer.add_scalar(\"Reward_1_1\", predicted_rewards[0][0], idx+len(train_dataloader)*epoch)\n",
    "                writer.add_scalar(\"Reward_1_2\", predicted_rewards[0][1], idx+len(train_dataloader)*epoch)           \n",
    "                writer.add_scalar(\"Reward_1_3\", predicted_rewards[0][2], idx+len(train_dataloader)*epoch)             \n",
    "                writer.add_scalar(\"Reward_2_1\", predicted_rewards[1][0], idx+len(train_dataloader)*epoch)          \n",
    "                writer.add_scalar(\"Reward_2_2\", predicted_rewards[1][1], idx+len(train_dataloader)*epoch)         \n",
    "                writer.add_scalar(\"Reward_2_3\", predicted_rewards[1][2], idx+len(train_dataloader)*epoch)\n",
    "                checkpoint = {\n",
    "                    'model_state_dict': reward_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'step': idx,\n",
    "                    'loss': loss\n",
    "                }\n",
    "                torch.save(checkpoint, \"reward_model_checkpoint.pt\")\n",
    "            if idx%800==799:                            ##EVAL STEPS            \n",
    "                valid_loss = eval_epoch_reward_model(reward_model, valid_dataloader, criterion)\n",
    "\n",
    "                reward_model.train()\n",
    "                \n",
    "                print(f\"Valid Loss: {valid_loss:.4f}\")\n",
    "                writer.add_scalar('Loss/valid', valid_loss, idx+len(train_dataloader)*epoch)\n",
    "\n",
    "        ##END-OF-EPOCH EVAL\n",
    "        avg_loss = total_loss / num_samples\n",
    "        writer.add_scalar('Loss/train', avg_loss, idx+len(train_dataloader)*epoch)               \n",
    "        total_loss = 0.0\n",
    "        num_samples = 0.0\n",
    "            \n",
    "        valid_loss = eval_epoch_reward_model(reward_model, valid_dataloader, criterion)\n",
    "\n",
    "        writer.add_scalar('Loss/valid', valid_loss, idx+len(train_dataloader)*epoch)\n",
    "        print(f\"End Epoch Train Loss: {avg_loss:.4f}; Valid Loss: {valid_loss:.4f}\")\n",
    "    \n",
    "    return reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9389a18d-14cd-4f27-8392-a0760a84cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel(model)\n",
    "reward_model.to(device)\n",
    "reward_model.apply(lambda layer: torch.nn.init.xavier_uniform_(layer.weight, gain=2) if isinstance(layer, torch.nn.Linear) else None)\n",
    "\n",
    "# Initialize biases to zeros\n",
    "for layer in reward_model.children():\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "accumulation_steps = 4\n",
    "\n",
    "optimizer = AdamW(reward_model.parameters(), lr=8e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c3f6c0-a9e7-4930-aa93-6bf4140341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, _LRScheduler\n",
    "from torch import optim\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            warmup_factor = float(self.last_epoch) / float(max(1, self.warmup_steps))\n",
    "            return [base_lr * warmup_factor for base_lr in self.base_lrs]\n",
    "        return self.base_lrs\n",
    "\n",
    "warmup_steps = int(800/accumulation_steps)\n",
    "warmup_scheduler = WarmUpLR(optimizer, warmup_steps)\n",
    "\n",
    "# Define a step scheduler after warm-up\n",
    "step_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(800/accumulation_steps), gamma=0.95)\n",
    "\n",
    "# Combine warm-up with step scheduler\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, step_scheduler], milestones=[warmup_steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac76c8e-0a18-41f9-96d9-299ebebb8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_reward_model = train_reward_model(reward_model, train_dataloader, valid_dataloader, optimizer, criterion, scheduler, accumulation_steps, epochs=1)\n",
    "\n",
    "# torch.save(fin_reward_model, \"reward_model_saved_final.pt\")\n",
    "# model.save_pretrained(\"path_to_save_model.pt\")\n",
    "# tokenizer.save_pretrained(\"path_to_save_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d6387-06ae-420c-9088-302941f70803",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fin_reward_model.state_dict(), \"reward_model_saved_state_dict.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6eba3-e729-43dd-92fc-5501f00383c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fin_reward_model, \"reward_model_saved_full.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
